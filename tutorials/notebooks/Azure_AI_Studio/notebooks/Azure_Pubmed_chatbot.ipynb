{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2edc6187-82ae-44e2-852f-2ad2712c93aa",
   "metadata": {},
   "source": [
    "# Creating a PubMed Chatbot on Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecea2ad-7c65-4367-87e1-b021167c3a1d",
   "metadata": {},
   "source": [
    "For this tutorial we are creating a PubMed chatbot that will answer questions based on information gathered from documents we have provided via an index. The model we will be using today is the pretrained **OpenAI** model. Most Azure command line tools are already installed and it is recommended to use the **AzureML** kernel in your Jupyter notebook.\n",
    "\n",
    "This tutorial will go over the following topics:\n",
    "- Introduce langchain\n",
    "- Explain the differences between zero-shot, one-shot, and few-shot prompting\n",
    "- Practice using different document retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01e74b-b5b4-4be9-b16e-ec55419318ef",
   "metadata": {},
   "source": [
    "### Optional: Deploy the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd13e7-afc9-416b-94dc-418a93e14587",
   "metadata": {},
   "source": [
    "In this tutorial we will be using Azure OpenAI which you can learn how to deploy [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=cli). This tutorial utilizes the model **gpt-35-turbo** version 0301 and the embeddings model **text-embedding-ada-002** version 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e3ab1-5f7e-4028-a66f-9619926a2afd",
   "metadata": {},
   "source": [
    "## PubMed API vs Azure AI Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a820eea-1538-4f40-86c4-eb14fe09e127",
   "metadata": {},
   "source": [
    "Our chatbot will rely on documents to answer our questions to do so we are supplying it a **vector index**. A vector index or index is a data structure that enables fast and accurate search and retrieval of vector embeddings from a large dataset of objects. We will be working with two options for our index: PubMed API vs Azure AI Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314b115-9433-460d-b275-78aa50f0a858",
   "metadata": {},
   "source": [
    "**What is the difference?**\n",
    "\n",
    "The **PubMed API** is provided free by langchain to connect your model to more than **35 million citations** for biomedical literature from MEDLINE, life science journals, and online books. The langchain package for PubMed is already a retriever meaning that just simply using this tool will give the ability for our chatbot to retrieve documents. \n",
    "\n",
    "**Azure AI Search** (formally known as Azure Cognitive Search) is a vector store from Azure that allows the user more **security and control** on which documents you wish to supply to your model. AI Search is a vector store or database that stores the **embeddings** of your documents and the metadata. It can also act as a retriever by using the langchain tool **AzureCognitiveSearchRetriever** which will be implementing the RAG method on the back end. **RAG** stands for **Retrieval-augmented generation** it is a method or technique that **indexes documents** by first loading them in, splitting them into chucks (making it easier for our model to search for relevant splits), embedding the splits, then storing them in a vector store. The next steps in RAG are based on the question you ask your chatbot. If we were to ask it \"What is a cell?\" the vector store will be searched by a retriever to find relevant splits that have to do with our question, thus **retrieving relevant documents**. And finally our chatbot will **generate an answer** that makes sense of what a cell is, as part of the answer it will also point out which source documents it used to create the answer.\n",
    "\n",
    "We will be exploring both methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf1690d-e93d-4cd3-89c6-8d06b5a071a8",
   "metadata": {},
   "source": [
    "## Setting up Azure AI Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6330ddf-7972-4451-9fcb-98cf83f5d118",
   "metadata": {},
   "source": [
    "If you choose to use Azure AI Search to supply documents to your model follow the instructions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b23ad-8809-4954-a4df-2ff3b8d9ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'langchain' 'unstructured' 'tiktoken'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abddde62-f269-454e-bea6-538bd4267277",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code AVT4CBLPV to authenticate.\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"14b77578-9773-42d5-8507-251ca2dc2b06\",\n",
      "    \"id\": \"f2b12bdb-662b-48ea-9dbb-c157b4dbc573\",\n",
      "    \"isDefault\": true,\n",
      "    \"managedByTenants\": [],\n",
      "    \"name\": \"NIH.CIT.CS.CloudLab.Azure_Test_01\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"14b77578-9773-42d5-8507-251ca2dc2b06\",\n",
      "    \"user\": {\n",
      "      \"name\": \"yosufzaizb@nih.gov\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#Authenticate to use azure cli\n",
    "! az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c1803b-829a-4256-a88c-1f4b57372ba2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (12.13.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-storage-blob) (1.26.4)\n",
      "Requirement already satisfied: msrest>=0.6.21 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-storage-blob) (0.7.1)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-storage-blob) (38.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.23.1->azure-storage-blob) (4.6.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.23.1->azure-storage-blob) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.23.1->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest>=0.6.21->azure-storage-blob) (2022.9.24)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest>=0.6.21->azure-storage-blob) (1.3.1)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest>=0.6.21->azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->azure-storage-blob) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->azure-storage-blob) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->azure-storage-blob) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-storage-blob) (3.2.2)\n",
      "Requirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n"
     ]
    }
   ],
   "source": [
    "#uncomment if update is needed\n",
    "#! pip install -U \"azure-storage-blob\" \"azure-search-documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428d5bc-76e3-4ee1-891b-0bc190c0ae2f",
   "metadata": {},
   "source": [
    "### Setting up our Storage Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b93a90-ff0b-430d-a5f4-4640bfb77b38",
   "metadata": {},
   "source": [
    "The first step will be to create a container that we will later use as our data source for our index. Set your storage account name, location, and container name variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "43cdc419-25e5-4ba8-b836-a13b2ad77a26",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1701806019922
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "location = 'eastus2'\n",
    "container_name = 'pubmed-chatbot-resources'\n",
    "\n",
    "#this should be the same as the one you used to set up your workspace\n",
    "resource_group = 'CloudLab'\n",
    "\n",
    "# storage_account_name can be found by going to Azure Machine Learning Workspace > Storage \n",
    "#or you can uncomment and run the command below to list the storage accounts names within your resource group\n",
    "storage_account_name = 'zycloudlab8923501391'\n",
    "\n",
    "#! az storage account list --resource-group {resource_group} --query \"[].{name:name}\" --output tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568fcc3-24a7-4f5d-9798-9016468a30ee",
   "metadata": {},
   "source": [
    "Create your container within your storage account running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf6c373-a0ee-40c6-a5c6-6841c58cc3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "There are no credentials provided in your command and environment, we will query for account key for your storage account.\n",
      "It is recommended to provide --connection-string, --account-key or --sas-token in your command as credentials.\n",
      "\n",
      "You also can add `--auth-mode login` in your command to use Azure Active Directory (Azure AD) for authorization if your login account is assigned required RBAC roles.\n",
      "For more information about RBAC roles in storage, visit https://docs.microsoft.com/azure/storage/common/storage-auth-aad-rbac-cli.\n",
      "\n",
      "In addition, setting the corresponding environment variables can avoid inputting credentials in your command. Please use --help to get more information about environment variable usage.\u001b[0m\n",
      "{\n",
      "  \"created\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! az storage container create -n {container_name} --account-name {storage_account_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adafdba-4c4d-4b96-b9bb-33143a72eafc",
   "metadata": {},
   "source": [
    "Run the command below to list the key values of your storage account. The key values will be saved to a json file for protection. We will need one of these keys to create a SAS token that gives us temporary access and permissions to add objects to our container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6414395e-80b1-4736-b116-70d82675b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az storage account keys list -g CloudLab -n {storage_account_name} > keys.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eb2a94da-40de-4b69-8de9-e001b4ea98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('keys.json', mode='r') as f:\n",
    "    data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "92cebae7-ddbb-4763-8d50-dd2c9f512696",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=data[0]['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec2e81-f5c1-43f1-8db0-769069acf9f7",
   "metadata": {},
   "source": [
    "Now we can create our SAS token that will last for 2 hour. Here we are giving our token the ability to read, write, list, add, and create objects (blobs) within our container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4b311d9b-1713-4699-ab62-b228d1decc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your SAS token\n",
    "from datetime import datetime, timedelta\n",
    "from azure.storage.blob import BlobServiceClient, generate_account_sas, ResourceTypes, AccountSasPermissions\n",
    "start_time = datetime.utcnow()\n",
    "expiry_time = start_time + timedelta(hours=2)\n",
    "sas_token = generate_account_sas(\n",
    "    account_name=storage_account_name,\n",
    "    container_name=container_name,\n",
    "    account_key=key,\n",
    "    resource_types=ResourceTypes(object=True),\n",
    "    permission=AccountSasPermissions(read=True, write=True, delete=True, list=True, add=True, create=True),\n",
    "    expiry=expiry_time,\n",
    "    start=start_time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900efd36-371b-4400-9a9f-fffd1bc14cce",
   "metadata": {},
   "source": [
    "### Gathering our Docs for our Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c9de7-4a06-4f85-b9ff-c8c9e51f8c70",
   "metadata": {},
   "source": [
    "AWS marketplace has PubMed database named **PubMed Central® (PMC)** that contains free full-text archive of biomedical and life sciences journal article at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM). We will be subsetting this database to add documents to our AI Search Index. Ensure that you have the correct permissions to allow your environment to connect to containers and AI Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad30ba-cee8-47f9-bc1e-ece8961ac66a",
   "metadata": {},
   "source": [
    "Here we are downloading the metadata file from the PMC index directory, this will list all of the articles within the PMC bucket and their paths. We will use this to subset the database into our own bucket. Here we are using curl to connect to the public AWS s3 bucket where the metadata and documents are originally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b395e34-062d-4f77-afee-3601d471954a",
   "metadata": {
    "gather": {
     "logged": 1701794361537
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  552M  100  552M    0     0  24.2M      0  0:00:22  0:00:22 --:--:-- 24.5M\n"
     ]
    }
   ],
   "source": [
    "#download the metadata file\n",
    "!curl -O http://pmc-oa-opendata.s3.amazonaws.com/oa_comm/txt/metadata/csv/oa_comm.filelist.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a8595a-767f-4cad-9273-62d8e2cf60d1",
   "metadata": {},
   "source": [
    "We only want the metadata of the first 100 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c26b0f29-2b07-43a6-800d-4aa5e957fe52",
   "metadata": {
    "gather": {
     "logged": 1701794425470
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import the file as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('oa_comm.filelist.csv')\n",
    "#first 100 files\n",
    "first_100=df[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1ae93-450e-4c79-83cc-ea46a1b507c1",
   "metadata": {},
   "source": [
    "Lets look at our metadata! We can see that the s3 bucket path to the files are under the **Key** column this is what we will use to loop through the PMC bucket and copy the first 100 files to our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff77b2aa-ed1b-4d27-8163-fdaa7a304582",
   "metadata": {
    "gather": {
     "logged": 1701794430114
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>ETag</th>\n",
       "      <th>Article Citation</th>\n",
       "      <th>AccessionID</th>\n",
       "      <th>Last Updated UTC (YYYY-MM-DD HH:MM:SS)</th>\n",
       "      <th>PMID</th>\n",
       "      <th>License</th>\n",
       "      <th>Retracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oa_comm/txt/all/PMC10000000.txt</td>\n",
       "      <td>5eaca73009b45b0860efad0a38f383a9</td>\n",
       "      <td>Chic Med Exam. 1867 Jan; 8(1):54b-64</td>\n",
       "      <td>PMC10000000</td>\n",
       "      <td>2023-07-20 23:18:08</td>\n",
       "      <td>37471719</td>\n",
       "      <td>CC0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Key                              ETag  \\\n",
       "0  oa_comm/txt/all/PMC10000000.txt  5eaca73009b45b0860efad0a38f383a9   \n",
       "\n",
       "                       Article Citation  AccessionID  \\\n",
       "0  Chic Med Exam. 1867 Jan; 8(1):54b-64  PMC10000000   \n",
       "\n",
       "  Last Updated UTC (YYYY-MM-DD HH:MM:SS)      PMID License Retracted  \n",
       "0                    2023-07-20 23:18:08  37471719     CC0        no  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5f36a-239c-4c15-80ab-f896d45849d3",
   "metadata": {},
   "source": [
    "The following commands uses `azcopy`, a tool that allows you to copy objects from AWS s3 buckets. The for loop we created will gather the location of each document with in AWS s3 bucket and save the documents to our container in the form of a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d63a7e2-dbf1-49ec-bc84-b8c2c8bde62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "#gather path to files in bucket\n",
    "for i in first_100['Key']:\n",
    "    doc_name=i.split(r'/')[-1]\n",
    "    os.system(f'azcopy copy \"https://s3.amazonaws.com/pmc-oa-opendata/{i}\" \"https://{storage_account_name}.blob.core.windows.net/{container_name}/{doc_name}?{sas_token}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928de2ca-010a-4087-82a7-e548f84f3d95",
   "metadata": {},
   "source": [
    "If you run into any errors make sure you have the `Storage Blob Data Contributor` role assigned to your storage account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adf90e-e88b-4631-b860-81c2ea347786",
   "metadata": {},
   "source": [
    "The command below sees if our files have any metadata already associated with them. If your data does not have metadata you can add it to your blob following the section **Adding Metadata to Our Data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d2831bf-babd-45cf-9641-a34e1b9d7c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Accession_id\": \"PMC10000000\",\n",
      "  \"Aiid\": \"10000000\",\n",
      "  \"Citation\": \"Chic Med Exam. 1867 Jan; 8(1):54b-64\",\n",
      "  \"License\": \"CC0\",\n",
      "  \"Md5\": \"5eaca73009b45b0860efad0a38f383a9\",\n",
      "  \"Pmid\": \"37471719\",\n",
      "  \"Retracted\": \"no\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! az storage blob metadata show --container-name {container_name} --account-name {storage_account_name} --account-key {key} --name 'PMC10000000.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613cef7d-d0aa-42a8-a46e-7fd1f5c48c3b",
   "metadata": {},
   "source": [
    "### Optional: Adding Metadata to Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6b7cf-decf-4e1d-8a36-86031cc64faf",
   "metadata": {},
   "source": [
    "Inorder to add metadata to our metadata keys can not have spaces and the value need to be strings. Here we are making a new dataframe with wanted columns for our metadata, these columns are from the `first_100` variable we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "22b9579b-bde9-4e57-bad6-700c7ee73645",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_table = first_100[['Article Citation', 'AccessionID', 'PMID']].copy()\n",
    "#make sure that all keys and values are strings the blob metadata with not beable to parse through our metadata if it is a integer\n",
    "metadata_table['PMID'] = metadata_table['PMID'].apply(str)\n",
    "metadata_table.rename(columns={'Article Citation': 'Article_Citation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c78e0-18d3-4162-9e58-c790ad85f76f",
   "metadata": {},
   "source": [
    "Transform our table into a dictionary to add to our blob metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ae31175-f664-413d-8d2f-ecaef67038dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = metadata_table.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd8101-c635-43a0-9645-1115b32eb037",
   "metadata": {},
   "source": [
    "Lets look at our metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5a9de234-c83a-4d94-87b8-3bd51fb0c531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Article_Citation': 'Chic Med Exam. 1867 Jan; 8(1):54b-64',\n",
       " 'AccessionID': 'PMC10000000',\n",
       " 'PMID': '37471719'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd877ef2-155f-4fe2-b0c1-8e45293196e2",
   "metadata": {},
   "source": [
    "Now that we have our metadata variables set we can connect to our container and the blobs within it by using a **BlobServiceClient**. This client service uses our storage account endpoint and our SAS token. Then we will construct a for loop that loops through the 'first_100' dataframe to gather our document name (which is also the blob name).\n",
    "\n",
    "Next it will do the following:\n",
    "- Gather the metadata (if any exists) of the blob\n",
    "- Update the metadata as the new metadata record we created 'metadata_dict'\n",
    "- Set the metadata on the blob. Although we have updated the metadata it will not save on your blob unless you set it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "46767760-8794-4860-9468-6b2d6b72022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "blob_service = BlobServiceClient(account_url=f'https://{storage_account_name}.blob.core.windows.net', credential=sas_token)\n",
    "\n",
    "for i in range(len(first_100['Key'])):\n",
    "    document_name = first_100['Key'][i].split(\"/\")[-1]  \n",
    "    blob_client = blob_service.get_blob_client(container=container_name, blob=document_name)\n",
    "    # gather metadata properties for that blob\n",
    "    blob_metadata = blob_client.get_blob_properties().metadata\n",
    "    # Update blob metadata\n",
    "    more_blob_metadata = metadata_dict[i]\n",
    "    blob_metadata.update(more_blob_metadata)\n",
    "\n",
    "    # Set metadata on the blob\n",
    "    blob_client.set_blob_metadata(metadata=blob_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf1733-a4f6-4d71-80d2-83089d6dd3f6",
   "metadata": {},
   "source": [
    "Lets check the metadata of one of our blobs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc7caf8-f021-4406-93bb-e4834a17cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"test\": \"test1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! az storage blob metadata show --container-name {container_name} --account-name {storage_account_name} --account-key {key} --name 'PMC10000000.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b396c8-baa9-44d6-948c-2326dc514839",
   "metadata": {},
   "source": [
    "### Creating an Azure AI Search Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fa941-bf59-4cae-9aa8-2f2741f3a1b1",
   "metadata": {},
   "source": [
    "To create our AI Search index, we will first need to create a search service, and request to create the free SKU to hold all our documents in our vector store. The **free** tier allows you to hold 50MB of data and 3 indexes, and indexers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "63226024-d03e-4fa0-9557-2f18fec07bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_name = 'pubmed-search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9458fa-3c0c-4249-a8bd-fd86f9bee8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K{- Starting ..\n",
      "  \"authOptions\": {\n",
      "    \"aadOrApiKey\": null,\n",
      "    \"apiKeyOnly\": {}\n",
      "  },\n",
      "  \"disableLocalAuth\": false,\n",
      "  \"encryptionWithCmk\": {\n",
      "    \"encryptionComplianceStatus\": \"Compliant\",\n",
      "    \"enforcement\": \"Unspecified\"\n",
      "  },\n",
      "  \"hostingMode\": \"default\",\n",
      "  \"id\": \"/subscriptions/f2b12bdb-662b-48ea-9dbb-c157b4dbc573/resourceGroups/CloudLab/providers/Microsoft.Search/searchServices/pubmed-search\",\n",
      "  \"identity\": null,\n",
      "  \"location\": \"East US 2\",\n",
      "  \"name\": \"pubmed-search\",\n",
      "  \"networkRuleSet\": {\n",
      "    \"bypass\": \"None\",\n",
      "    \"ipRules\": []\n",
      "  },\n",
      "  \"partitionCount\": 1,\n",
      "  \"privateEndpointConnections\": [],\n",
      "  \"provisioningState\": \"succeeded\",\n",
      "  \"publicNetworkAccess\": \"Enabled\",\n",
      "  \"replicaCount\": 1,\n",
      "  \"resourceGroup\": \"CloudLab\",\n",
      "  \"sharedPrivateLinkResources\": [],\n",
      "  \"sku\": {\n",
      "    \"name\": \"standard\"\n",
      "  },\n",
      "  \"status\": \"running\",\n",
      "  \"statusDetails\": \"\",\n",
      "  \"tags\": null,\n",
      "  \"type\": \"Microsoft.Search/searchServices\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! az search service create --name {service_name} --sku free --location {location} --resource-group {resource_group} --partition-count 1 --replica-count 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea51b2-6511-4ae4-ba9d-963a861376cd",
   "metadata": {},
   "source": [
    "Below will list the admin keys, select one of them to use for adding objects to our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "95382baa-abb9-4ad1-b1db-11cb9a606b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! az search admin-key show --resource-group {resource_group} --service-name {service_name} > keys.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c2455-9e61-4568-b2f1-03546c1f9878",
   "metadata": {},
   "source": [
    "Save one of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1dde166b-d342-400a-ba1d-23436e1938ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keys.json', mode='r') as f:\n",
    "    data = json.load(f)\n",
    "search_key = data[\"primaryKey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbde69-5a23-45a8-a000-9952824d973a",
   "metadata": {},
   "source": [
    "Now we can create our index using a SearchClient which will allow us to also define our fields within our index. Depending on the size of your documents you may need to split your document in chucks so that it fits within the token size of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5304e-8e14-485c-b452-e5af2da95e01",
   "metadata": {},
   "source": [
    "### Creating An Index and Loading Small Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ae3c4-ddec-473d-98e9-034e58542968",
   "metadata": {},
   "source": [
    "Here we can create an index that connects our blobs in our container using an **Indexer** and a **Data Container**.\n",
    "\n",
    "**Warning:** This dataset contains large documents the below steps are only meant to show you how the process would go with smaller documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c0540581-7b16-49af-8c67-a3bbd8a247d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndex,\n",
    "    SearchIndexer\n",
    ")\n",
    "\n",
    "endpoint = \"https://{}.search.windows.net/\".format(service_name)\n",
    "index_client = SearchIndexClient(endpoint, AzureKeyCredential(search_key))\n",
    "indexers_client = SearchIndexerClient(endpoint, AzureKeyCredential(search_key))\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584fad3-a07f-4263-97c3-475d774e87a1",
   "metadata": {},
   "source": [
    "Here we are stating our schema or fields before we create our index, these fields are from when we ran the `az storage blob metadata show` command after loading our blobs to our container.\n",
    "\n",
    "- **SimpleField:** A field that you can retrieve values but not search them this is ideal for keys which is a unique ID for each blob. Here we are setting the Md5 value as our key.\n",
    "- **SearchableField:** A field that allows you to retrieve and search values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2bddb00f-fa9e-4677-9d02-484b2eb5b02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes.models._index.SearchIndex at 0x7fe135db01c0>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_index_name = \"pubmed-index-smalldocs\"\n",
    "\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"Md5\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata_storage_path\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata_storage_name\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"Citation\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"Accession_id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"Pmid\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    )\n",
    "]\n",
    "#set our index values\n",
    "index = SearchIndex(name=s_index_name, fields=fields)\n",
    "#create our index\n",
    "index_client.create_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db17d58-1cc3-4a9c-8872-aeca6da86638",
   "metadata": {},
   "source": [
    "Now that our index is created we can create our **Data Container** which is the storage container that holds our documents. Once this is created we then create a **Indexer** that will link our data container and our index together, it also has the option to update our index if you were to add new blobs to your storage container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6ae3eddb-af30-40cc-95b9-bbba29109af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a datasource\n",
    "container = SearchIndexerDataContainer(name=container_name)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=\"pubmed-datasource\", type=\"azureblob\", connection_string=connection_string, container=container\n",
    ")\n",
    "data_source = indexers_client.create_data_source_connection(data_source_connection)\n",
    "\n",
    "# create an indexer\n",
    "indexer = SearchIndexer(\n",
    "    name=\"pubmed-indexer\", data_source_name=\"pubmed-datasource\", target_index_name=s_index_name\n",
    ")\n",
    "result = indexers_client.create_indexer(indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39913a9f-5026-4b50-91f9-2acd49d2999f",
   "metadata": {},
   "source": [
    "Wait about 5 mins for the index and indexer to sync."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4cbeb1-5ba2-4f56-bf8e-7b5875c29538",
   "metadata": {},
   "source": [
    "### Creating An Index and Loading Large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffaf25-4b54-498a-9b60-4fca4607e9e9",
   "metadata": {},
   "source": [
    "For our model to retrieve information from larger documents we need split the text in our documents into smaller chucks this will make it easier for our model to sift through our docs to retrieve information without going over the models token limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7acee9-fa89-4d45-b577-5f374103792f",
   "metadata": {},
   "source": [
    "If you remember before we mention **RAG** the process below follows this technique with the help of langchain tools. First we will add metadata to our docs, split our docs into chunks, and embed them. Then much like for smaller documents we will create an index, the fields in our index will be different compare to the small docs index. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1727147-3cf9-4f9f-a21e-6b33a2f5640d",
   "metadata": {},
   "source": [
    "#### Adding Metadata to Loaded Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa34e7b-99c7-4a2e-b73b-146636a98285",
   "metadata": {},
   "source": [
    "After we have our documents stored in our container we can start to load our files back. This step is necessary though redundant because we will need to embed our docs for our vector store and we need to attach metadata for each document. Although our blobs already have metadata attached to them unfortunately langchain document loader tools only retrieves the path of our files so we need to add them back. In this case we will be using **AzureBlobStorageContainerLoader** to load in the container that holds all of our documents.\n",
    "\n",
    "If your data is in a directory within your container add the `prefix` variable to the loader definition.\n",
    "\n",
    "When we load in our documents they will be set as a tuple that is named **Documents**. This tuple will contain two items:\n",
    "- **page content:** The text or content within our document\n",
    "- **metadata:** The associated metadata which for now will only hold the source (path) to our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a11c98f-463b-48fd-84f7-f2b99f87d992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents from pubmed-chatbot-resources\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={key}\"\n",
    "print(f\"Processing documents from {container_name}\")\n",
    "\n",
    "loader = AzureBlobStorageContainerLoader(\n",
    "    conn_str=connection_string, container=container_name\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ab068-2919-4d93-8711-15dd7eb19ada",
   "metadata": {},
   "source": [
    "Next we use our blob service client to retrieve our metadata from our blobs to add our metadata back to our loaded docs via a for loop. The metadata will consist of the source, title, and the original metadata fields from our blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a805af-98b1-4367-9aa9-de519e38bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents loaded (pre-chunking) = 100\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "blob_service = BlobServiceClient(account_url=f'https://{storage_account_name}.blob.core.windows.net', credential=sas_token)\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    #set metadata to variable\n",
    "    doc_md = documents[i].metadata\n",
    "    #gather document name from metadata to correct source formatting\n",
    "    document_name = doc_md[\"source\"].split(\"/\")[-1]\n",
    "    source = f'{container_name}/{document_name}'\n",
    "    #set the first two fields of our metadata\n",
    "    documents[i].metadata = {\"source\": source, \"title\": document_name}\n",
    "    #connect to our blob to gather the metadata\n",
    "    blob_client = blob_service.get_blob_client(container=container_name, blob=document_name)\n",
    "    other_metadata = blob_client.get_blob_properties().metadata\n",
    "    #add the blob metadata to our loaded documents\n",
    "    documents[i].metadata.update(other_metadata)\n",
    "print(f\"# of documents loaded (pre-chunking) = {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb10cd-4eb9-4678-aa18-b4f168f1d927",
   "metadata": {},
   "source": [
    "Lets look at our metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db06d582-b84d-4b9e-9ff1-1695e37bb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'pubmed-chatbot-resources/PMC10000000.txt', 'title': 'PMC10000000.txt', 'Accession_id': 'PMC10000000', 'Aiid': '10000000', 'Citation': 'Chic Med Exam. 1867 Jan; 8(1):54b-64', 'License': 'CC0', 'Md5': '5eaca73009b45b0860efad0a38f383a9', 'Pmid': '37471719', 'Retracted': 'no'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e21813-35fa-485a-ac2e-41d38676d87e",
   "metadata": {},
   "source": [
    "#### Splitting our Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb34dc-4b8d-4c92-9e64-f94926bd8793",
   "metadata": {},
   "source": [
    "Splitting our data into chucks will help our vector store parse through our data faster and efficiently.\n",
    "\n",
    "For this step we will be using langchains **RecursiveCharacterTextSplitter**. This text splitter allows us to set the size of each chunk, if the chunks should have any text overlap (this is to help the model bridge the some the chunks to make sense of them), and where best to separate texts. Each chunk will have the same metadata as the original document they came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ae5e1eb-b2df-465c-a37d-3ddbad526602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents loaded (pre-chunking) = 3354\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a small chunk size.\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    ")\n",
    "chunk = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"# of documents loaded (pre-chunking) = {len(chunk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70848ce-02ee-4c2c-9824-d231a4d9037a",
   "metadata": {},
   "source": [
    "lets look at one of our chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2ee4576-6d4b-4f70-8cf4-1f52abcb8208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Editorial\\n\\nCHICAGO MEDICAL SOCIETY.\\n\\nJan. 18th. The regular meeting of the Society was almost\\n\\nwholly occupied with the subject of City Health Regulations.\\n\\nDr. N. S. Davis, from the Special Committee on that subject,\\n\\nmade the following report, which, after free discussion, was\\n\\nadopted. The Committee was instructed to present the same\\n\\nto the Common Council, with the draft of an amendment to the\\n\\npresent laws, as indicated in the report.\\n\\nREPORT OF THE COMMITTEE ON THE HEALTH DEPARTMENT\\n\\nOF OUR CITY GOVERNMENT.\\n\\nPresented to the Chicago Medical Society, January 18th, 1867.\\n\\nThe subject assigned to your Committee for consideration, is\\n\\ncertainly one of the most important that can engage the atten-\\n\\ntion of the people of this, or any other large city. The adop-\\n\\ntion and enforcement of proper sanitary regulations is so closely\\n\\nconnected with the preservation of health and the prolongation\\n\\nof life and happiness, that no community can ne,gleet them\\n\\nwithout, sooner or later, suffering the severest penalties. In\\n\\nfulfilling the duties assigned to your Committee, three questions\\n\\nrequire careful consideration.\\n\\n1st. What are principles upon which a Municipal Health De-\\n\\npartment should be organized, in order to ensure the highest\\n\\ndegree of enlightinent and efficient action?\\n\\n2d. Is the present organization of the Health Department\\n\\nof our city defective; and if so, in what particulars?\\n\\n3d. What changes are necessary to remedy its defects, and\\n\\ncomplete its efficiency?\\n\\nIn answer to the first question, we would state, that the basis\\n\\nof a proper Health Organization, should consist of a Board of\\n\\nCommissioners, embodying in its members a high degree of ex-\\n\\necutive ability or business capacity, and a thorough knowledge\\n\\nof sanitary science, as applied to the preservation of the health\\n\\nof communities and the prevention of the spread of contagious\\n\\ndiseases. No board can embody these qualities, without be-\\n\\ning composed in part, at least, of thoroughly-educated physi-', metadata={'source': 'pubmed-chatbot-resources/PMC10000000.txt', 'title': 'PMC10000000.txt', 'Accession_id': 'PMC10000000', 'Aiid': '10000000', 'Citation': 'Chic Med Exam. 1867 Jan; 8(1):54b-64', 'License': 'CC0', 'Md5': '5eaca73009b45b0860efad0a38f383a9', 'Pmid': '37471719', 'Retracted': 'no'})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ae84a-3e21-41b0-85d0-7093c563bb90",
   "metadata": {},
   "source": [
    "#### Create an Index with a Vector Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10628e98-5486-4222-ad36-52ae4ad3a5c0",
   "metadata": {},
   "source": [
    "For our index we will be adding in a **content_vector** field which represents each chuck embedded. **Embedding** means that we are converting our text into a **numerical vectors** that will help our model find similar objects like documents that hold similar texts or find similar photos based on the numbers assigned to the object, basically capturing texts meaning and relationship through numbers. Depending on the model you choose you have to find an embedder that is compatible to our model. Since we are using a OpenAI model the compatible embedding model will be **text-embedding-ada-002**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11fbd018-4197-4641-bbc3-9feff8c4b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from azure.search.documents.indexes.models import SearchIndex\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    TextWeights,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    ComplexField\n",
    ")\n",
    "\n",
    "endpoint = \"https://{}.search.windows.net/\".format(service_name)\n",
    "index_client = SearchIndexClient(endpoint, AzureKeyCredential(search_key))\n",
    "\n",
    "#Setup embeddings model\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"01f0a2f985f44fe182f226a8b33aa03a\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://nih-openai-poc.openai.azure.com/\"\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    chunk_size=10, #processing our chunks in batches of 10\n",
    ")\n",
    "embedding_function = embeddings.embed_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8643cfd-861c-4d6b-92cf-f21f4e15ccfb",
   "metadata": {},
   "source": [
    "Now we can create our fields you will notice that they are different from the small documents fields. Because we are using langchain to add our chunks to our index all our metadata will be held in a field called metadata, the page_content will be held in content, and langchan will create ids for each chunk.\n",
    "\n",
    "Another field you might have noticed is the **content_vector** field this field will hold the content that has been embedded. To create this field we have to set a vector profile which dictates what algorithm we will have our vector store use to find text that are similar to each other (find the nearest neighbors) for this profile we will be using the **Hierarchical Navigable Small World (HNSW) algorithm**.\n",
    "\n",
    "- **SimpleField:** A field that you can retrieve values but not search them this is ideal for keys which is a unique ID for each blob. Here we are setting the id value as our key.\n",
    "- **SearchableField:** A field that allows you to retrieve and search values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b542ff8-221a-4e7a-8fca-d5ce09e5976d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes.models._index.SearchIndex at 0x7fe12f731790>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(embedding_function(\"Text\")),\n",
    "        vector_search_profile_name=\"my-vector-config\"\n",
    "    ),\n",
    "    SearchableField(name=\"metadata\", type=SearchFieldDataType.String, searchable=True),\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    profiles=[VectorSearchProfile(name=\"my-vector-config\", algorithm_configuration_name=\"my-algorithms-config\")],\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithms-config\")],\n",
    ")\n",
    "    \n",
    "l_index_name = \"pubmed-index-largedocs\"\n",
    "index = SearchIndex(name=l_index_name, fields=fields, vector_search=vector_search)\n",
    "index_client.create_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91998d-376f-4050-8080-50ee3c473ea6",
   "metadata": {},
   "source": [
    "Define your vector store for langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a1547a74-1727-4c14-8856-842b161fe201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vector_search_configuration is not a known attribute of class <class 'azure.search.documents.indexes.models._index.SearchField'> and will be ignored\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=endpoint,\n",
    "    azure_search_key=search_key,\n",
    "    index_name=l_index_name,\n",
    "    embedding_function=embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe658444-b194-4495-a53c-c39f98498178",
   "metadata": {},
   "source": [
    "#### Embedding and Adding Data to Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bfb5b-a3a6-4156-bca3-394774a94565",
   "metadata": {},
   "source": [
    "For our chunks to be read by our embedding model we need split the tuple within each chunk, remember that the chunks consists of tuple called **Document** that contains **page content** and **metadata**. The code below loops through the chunks and splits the page_content and metadata saving them as separate variable lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ba20bef-5d38-4a99-9374-7642563d8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in chunk]\n",
    "metadatas = [doc.metadata for doc in chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78fcd8d9-1e07-4413-bfbf-53347adf2bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Editorial\\n\\nCHICAGO MEDICAL SOCIETY.\\n\\nJan. 18th. The regular meeting of the Society was almost\\n\\nwholly occupied with the subject of City Health Regulations.\\n\\nDr. N. S. Davis, from the Special Committee on that subject,\\n\\nmade the following report, which, after free discussion, was\\n\\nadopted. The Committee was instructed to present the same\\n\\nto the Common Council, with the draft of an amendment to the\\n\\npresent laws, as indicated in the report.\\n\\nREPORT OF THE COMMITTEE ON THE HEALTH DEPARTMENT\\n\\nOF OUR CITY GOVERNMENT.\\n\\nPresented to the Chicago Medical Society, January 18th, 1867.\\n\\nThe subject assigned to your Committee for consideration, is\\n\\ncertainly one of the most important that can engage the atten-\\n\\ntion of the people of this, or any other large city. The adop-\\n\\ntion and enforcement of proper sanitary regulations is so closely\\n\\nconnected with the preservation of health and the prolongation\\n\\nof life and happiness, that no community can ne,gleet them\\n\\nwithout, sooner or later, suffering the severest penalties. In\\n\\nfulfilling the duties assigned to your Committee, three questions\\n\\nrequire careful consideration.\\n\\n1st. What are principles upon which a Municipal Health De-\\n\\npartment should be organized, in order to ensure the highest\\n\\ndegree of enlightinent and efficient action?\\n\\n2d. Is the present organization of the Health Department\\n\\nof our city defective; and if so, in what particulars?\\n\\n3d. What changes are necessary to remedy its defects, and\\n\\ncomplete its efficiency?\\n\\nIn answer to the first question, we would state, that the basis\\n\\nof a proper Health Organization, should consist of a Board of\\n\\nCommissioners, embodying in its members a high degree of ex-\\n\\necutive ability or business capacity, and a thorough knowledge\\n\\nof sanitary science, as applied to the preservation of the health\\n\\nof communities and the prevention of the spread of contagious\\n\\ndiseases. No board can embody these qualities, without be-\\n\\ning composed in part, at least, of thoroughly-educated physi-', metadata={'source': 'pubmed-chatbot-resources/PMC10000000.txt', 'title': 'PMC10000000.txt', 'Accession_id': 'PMC10000000', 'Aiid': '10000000', 'Citation': 'Chic Med Exam. 1867 Jan; 8(1):54b-64', 'License': 'CC0', 'Md5': '5eaca73009b45b0860efad0a38f383a9', 'Pmid': '37471719', 'Retracted': 'no'})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62095449-f2bd-4038-ac6a-e1569887680e",
   "metadata": {},
   "source": [
    "Finally we can upload our split content and metadata to our vector store! This may take 10 to 20 mins depending on how large your dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eff5e6-ff27-4ea8-9e6a-c0c5c05a245a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store.add_texts(texts=texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3bc6b-8c43-476f-a662-abda830dc2da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a Inference Script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2291e-109e-4120-ad10-5dbfd341a07b",
   "metadata": {},
   "source": [
    "Inorder for us to fluidly send input and receive outputs from our chatbot we need to create a **inference script** that will format inputs in a way that the chatbot can understand and format outputs in a way we can understand. We will also be supplying instructions to the chatbot through the script.\n",
    "\n",
    "Our script will utilize **langchain** tools and packages to enable our model to:\n",
    "- **Connect to sources of context** (e.g. providing our model with tasks and examples)\n",
    "- **Rely on reason** (e.g. instruct our model how to answer based on provided context)\n",
    "\n",
    "**Warning**: The following tools must be installed via your terminal `pip install \"langchain\" \"xmltodict\" \"openai\"` and the over all inference script must be run on the terminal via the command `python YOUR_SCRIPT.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad374085-c4b1-4083-85a5-90cba35846d6",
   "metadata": {},
   "source": [
    "The first part of our script will be to list all the tools that are required. \n",
    "-  **PubMedRetriever:** Utilizes the langchain retriever tool to specifically retrieve PubMed documents from the PubMed API.\n",
    "- **AzureCognitiveSearchRetriever:** Connects to Azure AI Search to be used as a langchain retriever tool by specifically retrieving embedded documents stored in your vector store.\n",
    "- **AzureChatOpenAI:** Connects to your deployed OpenAI model. \n",
    "- **ConversationalRetrievalChain:** Allows the user to construct a conversation with the model and retrieves the outputs while sending inputs to the model.\n",
    "- **PromptTemplate:** Allows the user to prompt the model to provide instructions, best method for zero and few shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ad48d-c6c8-421a-a48b-88e979d15b57",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "from langchain.retrievers import PubMedRetriever\n",
    "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f4c31-71cd-4f39-8bfc-de098bdbaafc",
   "metadata": {},
   "source": [
    "Second will build a class that will hold the functions we need to send inputs and retrieve outputs from our model. For the beginning of our class we will establish some colors to our text conversation with our chatbot which we will utilize later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbb901-f811-4b8e-a956-4c8c7f914ae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba36d057-5189-4075-a243-18996c6fc932",
   "metadata": {},
   "source": [
    "We need to extract environmental variables to connect to our Open AI model. They will be :\n",
    "- OpenAI Key\n",
    "- OpenAI Endpoint (url)\n",
    "- Open AI Deployment Name\n",
    "\n",
    "If you are using Azure Search AI instead of the PubMed API we need to create a function that will gather the necessary information to connect to our vector store, which will be the:\n",
    "- Azure Search AI Service Name\n",
    "- Azure Search AI Index Name\n",
    "- Azure Search AI API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a244a-7e71-40d3-ae78-8e166dd3c7ee",
   "metadata": {},
   "source": [
    "```python\n",
    "def build_chain():\n",
    "    os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    os.getenv(\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\")\n",
    "    os.getenv(\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\")\n",
    "    os.getenv(\"AZURE_COGNITIVE_SEARCH_API_KEY\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1012f-ed20-47b9-9162-924e03e836d5",
   "metadata": {},
   "source": [
    "Now we can define our OpenAI model that has been predeployed:\n",
    "- Temperature: Controls randomness, higher values increase diversity meaning a more unique response make the model to think harder. Must be a number from 0 to 1, 0 being less unique.\n",
    "- Max Output Tokens: Limit of tokens outputted by the model.(optional: can assign if you like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cadb1af-2c46-4ab1-92f9-6e0861f83324",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    temperature = 0.5\n",
    "    #max_tokens = 3000\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b4f91-0c64-459b-a6e9-8a955c0797c7",
   "metadata": {},
   "source": [
    "We specify what our retriever both the PubMed and Azure AI Search retriever are listed, please only add one per script.\n",
    "\n",
    "If using Azure AI Search we need to specify what we are retrieving for our model to review, in this case it is the **content** part of our scheme we set within our index. We also set **'top_k'** to 2 meaning that our retriever will retrieve 2 documents that are the most similar to our ask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c61724-23d3-4b49-8c72-cbd208bdb5df",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "retriever= PubMedRetriever()\n",
    "\n",
    "#only if using Azure AI Search as a retriever\n",
    "\n",
    "retriever = AzureCognitiveSearchRetriever(content_key=\"content\", top_k=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e464a-0931-444a-aa58-09ee0c4c9884",
   "metadata": {},
   "source": [
    "Here we are constructing our **prompt_template**, this is where we can try zero-shot or few-shot prompting. Only add one method per script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431051e-0e84-408e-9821-f50a9b88c9c1",
   "metadata": {},
   "source": [
    "#### Zero-shot prompting\n",
    "\n",
    "Zero-shot prompting does not require any additional training more so it gives a pre-trained language model a task or query to generate text (our output). The model relies on its general language understanding and the patterns it has learned during its training to produce relevant output. In our script we have connect our model to a **retriever** to make sure it gathers information from that retriever (this can be the PubMed API or Azure AI Search). \n",
    "\n",
    "See below that the task is more like instructions notifying our model they will be asked questions which it will answer based on the info of the scientific documents provided from the index provided (this can be the PubMed API or Vector Search index). All of this information is established as a **prompt template** for our model to receive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0316dc5-6274-4a5e-92e4-3d266ed6a4df",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "prompt_template = \"\"\"\n",
    "  Ignore everything before.\n",
    "  \n",
    "  Instructions:\n",
    "  I will provide you with research papers on a specific topic in English, and you will create a cumulative summary. \n",
    "  The summary should be concise and should accurately and objectively communicate the takeaway of the papers related to the topic. \n",
    "  You should not include any personal opinions or interpretations in your summary, but rather focus on objectively presenting the information from the papers. \n",
    "  Your summary should be written in your own words and ensure that your summary is clear, concise, and accurately reflects the content of the original papers.\n",
    "  \n",
    "  {question} Answer \"don't know\" if not present in the document. \n",
    "  {context}\n",
    "  Solution:\"\"\"\n",
    "  PROMPT = PromptTemplate(\n",
    "      template=prompt_template, input_variables=[\"context\", \"question\"],\n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe7032-8507-4d07-baab-1b3bf0e92074",
   "metadata": {},
   "source": [
    "#### One-shot and Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614ea04-e1f8-4941-ae16-4359f718f98f",
   "metadata": {},
   "source": [
    "One and few shot prompting are similar to one-shot prompting, in addition to giving our model a task just like before we have also supplied an example of how the our model structure our output.\n",
    "\n",
    "See below that we have implemented one-shot prompting to our script.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb9669-5b77-4d9b-9f4e-a0d3a18b0fae",
   "metadata": {},
   "source": [
    "```python\n",
    "prompt_template = \"\"\"\n",
    "  Instructions:\n",
    "  I will provide you with research papers on a specific topic in English, and you will create a cumulative summary. \n",
    "  The summary should be concise and should accurately and objectively communicate the takeaway of the papers related to the topic. \n",
    "  You should not include any personal opinions or interpretations in your summary, but rather focus on objectively presenting the information from the papers. \n",
    "  Your summary should be written in your own words and ensure that your summary is clear, concise, and accurately reflects the content of the original papers.\n",
    "  Examples:\n",
    "  Question: What is a cell?\n",
    "  Answer: '''\n",
    "  Cell, in biology, the basic membrane-bound unit that contains the fundamental molecules of life and of which all living things are composed. \n",
    "  Sources: \n",
    "  Chow, Christopher , Laskey, Ronald A. , Cooper, John A. , Alberts, Bruce M. , Staehelin, L. Andrew , \n",
    "  Stein, Wilfred D. , Bernfield, Merton R. , Lodish, Harvey F. , Cuffe, Michael and Slack, Jonathan M.W.. \n",
    "  \"cell\". Encyclopedia Britannica, 26 Sep. 2023, https://www.britannica.com/science/cell-biology. Accessed 9 November 2023.\n",
    "  '''\n",
    "  \n",
    "  {question} Answer \"don't know\" if not present in the document. \n",
    "  {context}\n",
    "  \n",
    "\n",
    "  \n",
    "  Solution:\"\"\"\n",
    "  PROMPT = PromptTemplate(\n",
    "      template=prompt_template, input_variables=[\"context\", \"question\"],\n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c66d53-97b2-46dc-a466-70a3d3bee4a7",
   "metadata": {},
   "source": [
    "The following set of commands control the chat history essentially telling the model to expect another question after it finishes answering the previous one. Follow up questions can contain references to past chat history so the **ConversationalRetrievalChain** combines the chat history and the followup question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question-answering chain to return a response.\n",
    "\n",
    "All of these pieces such as our conversational chain, prompt, and chat history are passed through a function called **run_chain** so that our model can return is response. We have also set the length of our chat history to one meaning that our model can only refer to the pervious conversation as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4d33b-60f2-4462-a8e6-bbce7f8a7b07",
   "metadata": {},
   "source": [
    "```python\n",
    "condense_qa_template = \"\"\"\n",
    "  Chat History:\n",
    "  {chat_history}\n",
    "  Here is a new question for you: {question}\n",
    "  Standalone question:\"\"\"\n",
    "  standalone_question_prompt = PromptTemplate.from_template(condense_qa_template)\n",
    " \n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, \n",
    "        retriever=retriever, \n",
    "        condense_question_prompt=standalone_question_prompt, \n",
    "        return_source_documents=True, \n",
    "        combine_docs_chain_kwargs={\"prompt\":PROMPT},\n",
    "        )\n",
    "      return qa\n",
    "\n",
    "def run_chain(chain, prompt: str, history=[]):\n",
    "    print(prompt)\n",
    "    return chain({\"question\": prompt, \"chat_history\": history})\n",
    "\n",
    "MAX_HISTORY_LENGTH = 1 #increase to refer to more pervious chats\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1ef8d-66fe-4f84-933b-af2d730bd114",
   "metadata": {},
   "source": [
    "The final part of our script utilizes our class and incorporates colors to add a bit of flare to our conversation with our model. The model when first initialized should greet the user asking **\"Hello! How can I help you?\"** then instructs the user to ask a question or exit the session **\"Ask a question, start a New search: or CTRL-D to exit.\"**. With every question submitted to the model it is labeled as a **new search** we then run the run_chain function to get the models response or answer and add the response to the **chat history**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6ef65-ced4-445e-875c-7fee3483b81d",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "  chat_history = []\n",
    "  qa = build_chain()\n",
    "  print(bcolors.OKBLUE + \"Hello! How can I help you?\" + bcolors.ENDC)\n",
    "  print(bcolors.OKCYAN + \"Ask a question, start a New search: or CTRL-D to exit.\" + bcolors.ENDC)\n",
    "  print(\">\", end=\" \", flush=True)\n",
    "  for query in sys.stdin:\n",
    "    if (query.strip().lower().startswith(\"new search:\")):\n",
    "      query = query.strip().lower().replace(\"new search:\",\"\")\n",
    "      chat_history = []\n",
    "    elif (len(chat_history) == MAX_HISTORY_LENGTH):\n",
    "      chat_history.pop(0)\n",
    "    result = run_chain(qa, query, chat_history)\n",
    "    chat_history.append((query, result[\"answer\"]))\n",
    "    print(bcolors.OKGREEN + result['answer'] + bcolors.ENDC)  \n",
    "     if 'source_documents' in result:\n",
    "      print(bcolors.OKGREEN + 'Sources:')\n",
    "      for d in result['source_documents']:\n",
    "            ###Use this for Azure Search AI\n",
    "            dict_meta=json.loads(d.metadata['metadata'])\n",
    "            print(dict_meta['source'])\n",
    "            ###\n",
    "            #Use this for PubMed retriever:\n",
    "            #print(\"PubMed UID: \"+d.metadata[\"uid\"])\n",
    "    print(bcolors.ENDC)\n",
    "    print(bcolors.OKCYAN + \"Ask a question, start a New search: or CTRL-D to exit.\" + bcolors.ENDC)\n",
    "    print(\">\", end=\" \", flush=True)\n",
    "  print(bcolors.OKBLUE + \"Bye\" + bcolors.ENDC)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcbd48-bb84-4310-b8eb-ad87850a8649",
   "metadata": {},
   "source": [
    "Running our script in the terminal will require us to export the following global variables before using the command `python NAME_OF_SCRIPT.py`. Example scripts are also ready to use within our "example_scripts" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97df23-6893-438d-8a67-cb7dbf83e407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#retreive info to allow langchain to connect to Azure Search AI\n",
    "print(service_name)\n",
    "print(l_index_name)\n",
    "print(s_index_name)\n",
    "print(s_index_name)\n",
    "print(search_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab00a3-54ff-4873-8d25-eaf8bd18a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter the global variables in your terminal\n",
    "export AZURE_OPENAI_API_KEY='<AZURE_OPENAI_API_KEY>' \\\n",
    "export AZURE_OPENAI_ENDPOINT='<AZURE_OPENAI_ENDPOINT>' \\\n",
    "export AZURE_OPENAI_DEPLOYMENT_NAME='<AZURE_OPENAI_DEPLOYMENT_NAME>' \\\n",
    "export AZURE_COGNITIVE_SEARCH_SERVICE_NAME='<AZURE_COGNITIVE_SEARCH_SERVICE_NAME>' \\\n",
    "export AZURE_COGNITIVE_SEARCH_INDEX_NAME='<AZURE_COGNITIVE_SEARCH_INDEX_NAME>' \\\n",
    "export AZURE_COGNITIVE_SEARCH_API_KEY='<AZURE_COGNITIVE_SEARCH_API_KEY>' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe127e6-c0b1-4e07-ad56-38c30a9bf858",
   "metadata": {
    "tags": []
   },
   "source": [
    "You should see similar results on the terminal. In this example we ask the chatbot summarize one of our documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8fb4b-e74f-4e8d-892b-0f913eff747d",
   "metadata": {},
   "source": [
    "![PubMed Chatbot Results](../../../../docs/images/azure_chatbot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178c1c6-368a-48c5-8beb-278443b685a2",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec06a34-dc47-453f-b519-424804fa2748",
   "metadata": {},
   "source": [
    "**Warning:** Dont forget to delete the resources we just made to avoid accruing additional costs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307bb17-757a-4579-a0d8-698eb1bb3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete search service this will also delete any indexes, datastore, and indexers\n",
    "! az search service delete --name {service_name} --resource-group {resource_group} -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cea0a-a8fc-494e-8ce4-afb65847a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete container\n",
    "! az storage container delete -n {container_name} --account-name {storage_account_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928d95d-d7ec-43f6-9135-79fcfc9520d9",
   "metadata": {},
   "source": [
    "Dont forget to also delete or undeploy your OpenAI model and embeddings model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7350f02-aaf2-444d-b32a-c414d7d857ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

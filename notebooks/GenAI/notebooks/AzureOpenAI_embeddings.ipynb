{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Azure OpenAI LLMs from a notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Models you deploy to Azure OpenAI can be accessed via API calls. This tutorial gives you the basics of creating local embeddings from custom data and querying over those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "We assume you have access to Azure AI Studio, Azure AI Search, and have already deployed an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "+ Get familiar with Azure OpenAI APIs\n",
    "+ Learn how to create embeddings from custom data\n",
    "+ Learn how to query over those embedings\n",
    "+ Learn how to access deployed LLMs outside of the Azure console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"openai\" \"requests\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a query on a local csv file by creating local embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import dotenv\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need to [deploy a new model](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal#deploy-a-model). You need to select and deploy `text-embedding-ada-0021`. If you get an error downstream about your model not being ready, give it up to five minutes for everything to sync. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we just use a microsoft example here, but you could theoretically use any csv file as long as you match the expected format of the downstream code. This example is a recent earning report given by the CEO of Microsoft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data file to be embedded\n",
    "df = pd.read_csv('microsoft-earnings.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set keys and configure Azure OpenAI\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"<YOUR OPENAI ENDPOINT>\"\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = \"<YOUR OPENAI KEY>\"\n",
    "\n",
    "#create embeddings functions to apply to a given column\n",
    "    \n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an **get_embedding** function to quickly transform our text into numerical vectors that represent the relationship or how similar they are to each other. Then with our **cosine_similarity** function we can gather the texts that are similar to each other based on the query we will submit soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word embeddings\n",
    "df['embedding'] = df['text'].apply(lambda x: get_embedding(x))\n",
    "df.to_csv('microsoft-earnings_embeddings.csv', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the embeddings. After each query you put into the little box, you need to rerun this cell to reset the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the embeddings .csv \n",
    "# convert elements in 'embedding' column back to numpy array\n",
    "df = pd.read_csv('microsoft-earnings_embeddings.csv')\n",
    "df['embedding'] = df['embedding'].apply(eval).apply(np.array)\n",
    "\n",
    "# caluculate user query embedding \n",
    "search_term = input(\"Enter a search term: \")\n",
    "if search_term:\n",
    "    search_term_vector = get_embedding(search_term, engine='text-embedding-ada-002')\n",
    "\n",
    "    # find similiarity between query and vectors \n",
    "    df['similarities'] = df['embedding'].apply(lambda x:cosine_similarity(x, search_term_vector))\n",
    "    df1 = df.sort_values(\"similarities\", ascending=False).head(5)\n",
    "\n",
    "    # output the response \n",
    "    print('\\n')\n",
    "    print('Answer: ', df1['text'].loc[df1.index[0]])\n",
    "    print('\\n')\n",
    "    print('Similarity Score: ', df1['similarities'].loc[df1.index[0]]) \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query your own data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the README, we show how to add your own data. Go to **Azure AI Search** click on the your service you created to fill in the AI search variable needed below. When you have done this, type in a query, and then similar to what we show above you will get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "deployment_id = \"<YOUR DEPLOYMENT ID>\" # Add your deployment ID here\n",
    "# Azure AI Search setup\n",
    "search_endpoint = \"https://{}.search.windows.net\".format(\"<YOUR SEARCH SERVICE NAME>\") # Add your Azure AI Search name here\n",
    "# This is different than the key from above, its the key for the AI search\n",
    "search_key = \"<YOUR SEARCH KEY>\"; # Add your Azure AI Search admin key here\n",
    "search_index_name = \"<YOUR SEARCH INDEX>\"; # Add your Azure AI Search index name here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the query, note that the query is defined in the block below within the `message` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    base_url=f\"{endpoint}/openai/deployments/{deployment_id}/extensions\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-08-01-preview\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=deployment_id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What were some of the phenotypic presentations of MPOX on patients with HIV?\",\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    "    extra_body={\n",
    "        \"dataSources\": [\n",
    "            {\n",
    "                \"type\": \"AzureCognitiveSearch\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"key\": search_key,\n",
    "                    \"indexName\": search_index_name\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook you learned how to feed a PDF document directly to an LLM that you deployed in the Azure console and summarize the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Make sure to shut down your Azure ML compute and if desired you can delete your deployed model on Azure OpenAI and Azure AI Search index and service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}

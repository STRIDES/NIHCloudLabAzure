{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access Azure OpenAI LLMs from a notebook "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "Models you deploy to Azure OpenAI can be accessed via API calls. This tutorial gives you the basics of creating local embeddings from custom data and querying over those."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "We assume you have access to Azure AI Studio and have already deployed an LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning objectives\n",
        "+ Get familiar with Azure OpenAI APIs\n",
        "+ Learn how to create embeddings from custom data\n",
        "+ Learn how to query over those embedings\n",
        "+ Learn how to access deployed LLMs outside of the Azure console"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696341373678
        }
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Run a query on a local csv file by creating local embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696365118786
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "You also need to [deploy a new model](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal#deploy-a-model). You need to select and deploy `text-embedding-ada-0021`. If you get an error downstream about your model not being ready, give it up to five minutes for everything to sync. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "For simplicity, we just use a microsoft example here, but you could theoretically use any csv file as long as you match the expected format of the downstream code. This example is a recent earning report given by the CEO of Microsoft. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696367383849
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# read the data file to be embedded\n",
        "df = pd.read_csv('microsoft-earnings.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696367387035
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# set keys and configure Azure OpenAI\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = \"<YOUR BASE URL>\"\n",
        "openai.api_version = \"2023-07-01-preview\"\n",
        "# get the key from the instructions in the README of this repo. \n",
        "#You can also just click View Code in the chat playground\n",
        "openai.api_key = \"<YOUR KEY>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696367395456
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# calculate word embeddings \n",
        "df['embedding'] = df['text'].apply(lambda x:get_embedding(x, engine='text-embedding-ada-002'))\n",
        "df.to_csv('microsoft-earnings_embeddings.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Query the embeddings. After each query you put into the little box, you need to rerun this cell to reset the query. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696346882392
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# read in the embeddings .csv \n",
        "# convert elements in 'embedding' column back to numpy array\n",
        "df = pd.read_csv('microsoft-earnings_embeddings.csv')\n",
        "df['embedding'] = df['embedding'].apply(eval).apply(np.array)\n",
        "\n",
        "# caluculate user query embedding \n",
        "search_term = input(\"Enter a search term: \")\n",
        "if search_term:\n",
        "    search_term_vector = get_embedding(search_term, engine='text-embedding-ada-002')\n",
        "\n",
        "    # find similiarity between query and vectors \n",
        "    df['similarities'] = df['embedding'].apply(lambda x:cosine_similarity(x, search_term_vector))\n",
        "    df1 = df.sort_values(\"similarities\", ascending=False).head(5)\n",
        "\n",
        "    # output the response \n",
        "    print('\\n')\n",
        "    print('Answer: ', df1['text'].loc[df1.index[0]])\n",
        "    print('\\n')\n",
        "    print('Similarity Score: ', df1['similarities'].loc[df1.index[0]]) \n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Query your own data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "In the README, we show how to add your own data. When you have done this, type in a query, and then similar to what we show for above, if you click **View Code** in the Chat Playground, it will show you all the metadata you need to fill in here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "openai.api_type = \"azure\"\n",
        "openai.api_version = \"2023-08-01-preview\"\n",
        "# Azure OpenAI setup\n",
        "openai.api_base = \"<YOUR BASE URL>\" # Add your endpoint here\n",
        "deployment_id = \"<YOUR DEPLOYMENT ID>\" # Add your deployment ID here\n",
        "# Azure Cognitive Search setup\n",
        "search_endpoint = \"<YOUR COG SEARCH BASE URL>\"; # Add your Azure Cognitive Search endpoint here\n",
        "# This is different than the key from above, its the key for the Cog search\n",
        "search_key = \"<YOUR SEARCH KEY>\"; # Add your Azure Cognitive Search admin key here\n",
        "search_index_name = \"<YOUR SEARCH INDEX>\"; # Add your Azure Cognitive Search index name here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now run the query, note that the query is defined in the block below, and will output in Json format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696353881797
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def setup_byod(deployment_id: str) -> None:\n",
        "    \"\"\"Sets up the OpenAI Python SDK to use your own data for the chat endpoint.\n",
        "\n",
        "    :param deployment_id: The deployment ID for the model to use with your own data.\n",
        "\n",
        "    To remove this configuration, simply set openai.requestssession to None.\n",
        "    \"\"\"\n",
        "\n",
        "    class BringYourOwnDataAdapter(requests.adapters.HTTPAdapter):\n",
        "\n",
        "        def send(self, request, **kwargs):\n",
        "            request.url = f\"{openai.api_base}/openai/deployments/{deployment_id}/extensions/chat/completions?api-version={openai.api_version}\"\n",
        "            return super().send(request, **kwargs)\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    # Mount a custom adapter which will use the extensions endpoint for any call using the given `deployment_id`\n",
        "    session.mount(\n",
        "        prefix=f\"{openai.api_base}/openai/deployments/{deployment_id}\",\n",
        "        adapter=BringYourOwnDataAdapter()\n",
        "    )\n",
        "\n",
        "    openai.requestssession = session\n",
        "\n",
        "setup_byod(deployment_id)\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What were some of the phenotypic presentations of MPOX on patients with HIV?\"}],\n",
        "    deployment_id=deployment_id,\n",
        "    dataSources=[  # camelCase is intentional, as this is the format the API expects\n",
        "        {\n",
        "            \"type\": \"AzureCognitiveSearch\",\n",
        "            \"parameters\": {\n",
        "                \"endpoint\": search_endpoint,\n",
        "                \"key\": search_key,\n",
        "                \"indexName\": search_index_name,\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(completion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Conclusion\n",
        "In this notebook you learned how to feed a PDF document directly to an LLM that you deployed in the Azure console and summarize the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean up\n",
        "Make sure to shut down your Azure ML compute and if desired you can delete your deployed model on Azure OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

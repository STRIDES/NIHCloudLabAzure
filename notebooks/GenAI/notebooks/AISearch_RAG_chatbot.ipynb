{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embedding and Indexing with Azure OpenAI and AI Search\n",
    "\n",
    "## Introduction  \n",
    "This notebook is designed to help developers build applications that utilize various Azure services to process and retrieve data. The main goal is to pull files from Azure Blob Storage, generate embeddings using Azure OpenAI, store these documents with custom metadata in an Azure AI Index, and then interact with the indexed data via Azure OpenAI.  \n",
    "### Objectives  \n",
    "1. **Vectorize**:  \n",
    "    - Pull files from Azure Blob Containers.  \n",
    "    - Generate embeddings using Azure OpenAI.  \n",
    "    - Store documents with custom metadata in an Azure AI Index.  \n",
    "2. **Retrieve**:  \n",
    "    - Chat over the data indexes with Azure OpenAI.  \n",
    "Each section of the notebook will focus on specific tasks and utilize the REST APIs provided by each Azure service to accomplish these tasks. By the end of this notebook, you will have a comprehensive understanding of how to integrate and use these Azure services to build a robust data processing and retrieval application.  \n",
    "### Prerequisites  \n",
    "Before proceeding with this notebook, please ensure that you have the following Azure services deployed and configured:  \n",
    "  \n",
    "1. **Azure OpenAI Service**:   \n",
    "    - Ensure that you have deployed both a GPT model and an Ada model within your Azure OpenAI instance.  \n",
    "2. **Azure AI Search**:   \n",
    "    - Your Azure AI Search service should be a minimum of the Basic tier to ensure compatibility with Azure OpenAI.  \n",
    "3. **Azure Blob Storage Account**:   \n",
    "    - You should have an Azure Blob Storage account with PDF files stored in a blob container. These files should be located in the `/search_documents` directory of the `GenAI` directory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup  \n",
    "This section will guide you through setting up the environment for the notebook. We will import the necessary libraries, load environment variables, and configure Azure AI Search parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Install Python libraries from requirements.txt\n",
    "\n",
    "To ensure all necessary Python libraries are installed in the virtual environment for this notebook, we will use `pip` to install the packages specified in the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt # Will Install all packages from the requirements.txt file into your .venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Import Necessary Libraries  \n",
    "Import all the packages installed in the virtual environment into our Python script. This is a crucial step as it makes the required functionalities available for the script to execute correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries  \n",
    "  \n",
    "# For handling file and directory operations  \n",
    "import os  \n",
    "  \n",
    "# For handling I/O operations  \n",
    "import io  \n",
    "  \n",
    "# For extracting text and tables from PDF files  \n",
    "import pdfplumber  \n",
    "  \n",
    "# For interacting with Azure Blob Storage  \n",
    "from azure.storage.blob import BlobServiceClient  \n",
    "  \n",
    "# For handling Azure credentials  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.identity import DefaultAzureCredential  \n",
    "  \n",
    "# For working with Azure Search service  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "  \n",
    "# For configuring search indexes and fields  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SimpleField,                   # Represents a simple field in an index  \n",
    "    SearchFieldDataType,           # Represents the data type of a field  \n",
    "    VectorSearch,                  # Enables vector search capabilities  \n",
    "    SearchIndex,                   # Represents a search index  \n",
    "    SearchableField,               # Represents a searchable field  \n",
    "    SearchField,                   # Represents a field in a search index  \n",
    "    VectorSearchProfile,           # Represents a vector search profile  \n",
    "    HnswAlgorithmConfiguration     # Configuration for HNSW algorithm in vector search  \n",
    ")  \n",
    "  \n",
    "# For loading environment variables from a .env file  \n",
    "from dotenv import load_dotenv  \n",
    "  \n",
    "# For utilizing OpenAI functionalities within Azure  \n",
    "from openai import AzureOpenAI  \n",
    "  \n",
    "# For tokenization tasks  \n",
    "import tiktoken  \n",
    "  \n",
    "# For regular expression operations  \n",
    "import re  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Load Environment Variables  \n",
    "Load the environment variables from a `.env` file. Ensure you have a `.env` file with the required Azure service credentials and configurations. This file should contain all necessary keys and connection strings to connect to your Azure services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables  \n",
    "load_dotenv()  \n",
    "  \n",
    "# Example .env file format:  \n",
    "# AZURE_OPENAI_VERSION=your_openai_version  \n",
    "# AZURE_OPENAI_BASE=your_openai_base_url  \n",
    "# AZURE_OPENAI_ENDPOINT=your_openai_endpoint  \n",
    "# AZURE_OPENAI_KEY=your_openai_key  \n",
    "# AZURE_GPT_DEPLOYMENT=your_gpt_deployment  \n",
    "# AZURE_EMBEDDINGS_DEPLOYMENT=your_embeddings_deployment  \n",
    "# AZURE_SEARCH_ENDPOINT=your_search_endpoint  \n",
    "# AZURE_SEARCH_ADMIN_KEY=your_search_admin_key  \n",
    "# AZURE_SEARCH_INDEX=your_search_index  \n",
    "# BLOB_CONTAINER_NAME=your_blob_container_name  \n",
    "# BLOB_CONNECTION_STRING=your_blob_connection_string  \n",
    "# BLOB_ACCOUNT_NAME=your_blob_account_name  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `load_dotenv()` function reads the key-value pairs from the .env file and adds them to the environment variables.\n",
    "- Replace the placeholder values in your .env file with your actual Azure service credentials and configuration details.\n",
    "- ***This step is crucial for securely managing your credentials and keeping them out of your main codebase.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Configure Azure AI Search Parameters\n",
    "Configure the Azure AI Search parameters using the loaded environment variables. This allows us to set up the necessary configurations for connecting to the Azure AI Search service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure AI Search parameters  \n",
    "search_endpoint = os.getenv('AZURE_SEARCH_ENDPOINT')  # Get the Azure Search endpoint from environment variables  \n",
    "search_key = os.getenv('AZURE_SEARCH_ADMIN_KEY')      # Get the Azure Search admin key from environment variables  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `os.getenv('AZURE_SEARCH_ENDPOINT')` retrieves the value of the AZURE_SEARCH_ENDPOINT environment variable, which contains the endpoint URL for your Azure Search service.\n",
    "- `os.getenv('AZURE_SEARCH_ADMIN_KEY')` retrieves the value of the AZURE_SEARCH_ADMIN_KEY environment variable, which contains the admin key for your Azure Search service.\n",
    "- These configurations are essential for authenticating and connecting to your Azure Search service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectorization \n",
    "  \n",
    "In this section, we will connect to Azure Blob Storage, process PDF documents into text chunks with metadata, generate embeddings using Azure OpenAI, and upload the data to Azure AI Search.  \n",
    "\n",
    "Objectives:\n",
    "1. Setup Function for Azure OpenAI\n",
    "2. Connecting to Azure Blob Storage\n",
    "3. Splitting Text with Metadata\n",
    "4. Loading Blob Content\n",
    "5. Vectorize Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setup Function for Azure OpenAI  \n",
    "This function sets up the Azure OpenAI instance using the provided API key, version, and endpoint from environment variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_azure_openai():  \n",
    "    \"\"\"  \n",
    "    Sets up Azure OpenAI.  \n",
    "    \"\"\"  \n",
    "    print(\"Setting up Azure OpenAI...\")  \n",
    "    azure_openai = AzureOpenAI(  \n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "        api_version=os.getenv('AZURE_OPENAI_VERSION'),  \n",
    "        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')  \n",
    "    )  \n",
    "    print(\"Azure OpenAI setup complete.\")  \n",
    "    return azure_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Connecting to Azure Blob Storage  \n",
    "The following function connects to the Azure Blob Storage using the provided connection string and container name from the environment variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_blob_storage():  \n",
    "    \"\"\"  \n",
    "    Connects to Azure Blob Storage.  \n",
    "    \"\"\"  \n",
    "    print(\"Connecting to Blob Storage...\")  \n",
    "    blob_service_client = BlobServiceClient.from_connection_string(os.getenv(\"BLOB_CONNECTION_STRING\"))  \n",
    "    container_client = blob_service_client.get_container_client(os.getenv(\"BLOB_CONTAINER_NAME\"))  \n",
    "    print(\"Connected to Blob Storage.\")  \n",
    "    return container_client "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Splitting Text with Metadata  \n",
    "Split the content from PDF files into chunks with associated metadata. The text will be split by a max token length with additional chunk overlap. This is useful for processing large documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_with_metadata(text, metadata, max_length=800, overlap=75, encoding_name='cl100k_base'):  \n",
    "    \"\"\"  \n",
    "    Splits the text into chunks with metadata.  \n",
    "    \"\"\"  \n",
    "    tokenizer = tiktoken.get_encoding(encoding_name)  \n",
    "    tokens = tokenizer.encode(text)  \n",
    "    chunks = []  \n",
    "    start = 0  \n",
    "    end = max_length  \n",
    "      \n",
    "    while start < len(tokens):  \n",
    "        chunk = tokens[start:end]  \n",
    "        chunk_text = tokenizer.decode(chunk)  \n",
    "        chunk_metadata = metadata.copy()  \n",
    "        chunk_metadata.update({  \n",
    "            'start_token': start,  \n",
    "            'end_token': end,  \n",
    "            'chunk_length': len(chunk),  \n",
    "            'chunk_text_preview': chunk_text[:50] + '...'  \n",
    "        })  \n",
    "        chunks.append({  \n",
    "            'text': chunk_text,  \n",
    "            'metadata': chunk_metadata  \n",
    "        })  \n",
    "        start = end - overlap  \n",
    "        end = start + max_length  \n",
    "      \n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***Tokenize Text***: The text is encoded into tokens using the tokenizer.\n",
    "2. ***Initialize Variables***: Set up initial indices for chunking.\n",
    "3. ***Create Chunks***: Loop through the tokens to create chunks:\n",
    "    - Extract a chunk of tokens.\n",
    "    - Decode the chunk back into text.\n",
    "    - Copy and update metadata with chunk-specific information.\n",
    "    - Append the chunk and its metadata to the list.\n",
    "4. ***Overlap Handling***: Move the start index back by the overlap amount to ensure chunks overlap as specified.\n",
    "\n",
    "**Key Params**:\n",
    "- ***Max Chunk Size (max_length)***: Each chunk will have a maximum of `max_length` tokens (default is 800 tokens).\n",
    "- ***Chunk Overlap (overlap)***: Consecutive chunks will overlap by `overlap` tokens (default is 75 tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Loading Blob Content  \n",
    "Load and extracts the content of a PDF file from the Azure Blob Storage client.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blob_content(blob_client):  \n",
    "    \"\"\"  \n",
    "    Loads and returns the content of the PDF blob.  \n",
    "    \"\"\"  \n",
    "    blob_name = blob_client.blob_name  \n",
    "    if not blob_name.lower().endswith('.pdf'):  \n",
    "        raise ValueError(f\"Blob {blob_name} is not a PDF file.\")  \n",
    "      \n",
    "    blob_data = blob_client.download_blob().readall()  \n",
    "    pdf_stream = io.BytesIO(blob_data)  \n",
    "    document_text = \"\"  \n",
    "      \n",
    "    with pdfplumber.open(pdf_stream) as pdf:  \n",
    "        for page in pdf.pages:  \n",
    "            document_text += page.extract_text() + \"\\n\"  \n",
    "      \n",
    "    return document_text  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Check File Type**:\n",
    "    - The function first checks if the blob is a PDF file by verifying the file extension.\n",
    "    - If the file is not a PDF, it raises a `ValueError`.\n",
    "2. **Download Blob Content**:\n",
    "    - The blob content is downloaded and read into `blob_data`.\n",
    "3. **Convert to Stream**:\n",
    "    - The blob data is converted into a byte stream using `io.BytesIO`.\n",
    "4. **Extract Text from PDF**:\n",
    "    - The PDF is opened using `pdfplumber`.\n",
    "    - Text is extracted from each page of the PDF and concatenated into `document_text`.\n",
    "5. **Return Document Text**:\n",
    "    - The function returns the extracted text from the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Vectorize Function  \n",
    "Create a function that orchestrates the vector workflow. This function will connect to Azure services, processes blobs, generate embeddings, and upload the data to Azure AI Search index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize():  \n",
    "    \"\"\"  \n",
    "    Main function that orchestrates the vector workflow.  \n",
    "    \"\"\"  \n",
    "    azure_openai = setup_azure_openai()  \n",
    "    container_client = connect_to_blob_storage()  \n",
    "      \n",
    "    # Read and chunk documents with metadata  \n",
    "    print(\"Listing blobs in container...\")  \n",
    "    blob_list = container_client.list_blobs()  \n",
    "    documents = []  \n",
    "    for blob in blob_list:  \n",
    "        if not blob.name.lower().endswith('.pdf'):  \n",
    "            print(f\"Skipping non-PDF blob: {blob.name}\")  \n",
    "            continue  \n",
    "          \n",
    "        print(f\"Processing blob: {blob.name}\")  \n",
    "        blob_client = container_client.get_blob_client(blob)  \n",
    "        try:  \n",
    "            document = load_blob_content(blob_client)  \n",
    "            document_link = f'https://{os.getenv(\"BLOB_ACCOUNT_NAME\")}.blob.core.windows.net/{os.getenv(\"BLOB_CONTAINER_NAME\")}/{blob.name}'  \n",
    "              \n",
    "            metadata = {\"blob_name\": blob.name, \"document_link\": document_link}  \n",
    "            chunks = split_text_with_metadata(document, metadata)  \n",
    "            documents.extend(chunks)  \n",
    "        except Exception as e:  \n",
    "            print(f\"Failed to process blob {blob.name}: {e}\")  \n",
    "      \n",
    "    print(\"Blobs processed and documents chunked.\")  \n",
    "      \n",
    "    # Generate embeddings  \n",
    "    print(\"Generating embeddings...\")  \n",
    "    embeddings = []  \n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")  \n",
    "    max_tokens = 8192  \n",
    "    for i, doc in enumerate(documents):  \n",
    "        print(f\"Processing chunk {i + 1}/{len(documents)}\")  \n",
    "        print(f\"Chunk text: {doc['text']}\\n\")  \n",
    "        tokens = tokenizer.encode(doc[\"text\"])  \n",
    "        if len(tokens) > max_tokens:  \n",
    "            print(f\"Skipping document chunk {i + 1} with {len(tokens)} tokens, exceeding max limit of {max_tokens}.\")  \n",
    "            continue  \n",
    "        response = azure_openai.embeddings.create(input=doc[\"text\"], model=os.getenv(\"AZURE_EMBEDDINGS_DEPLOYMENT\"))  \n",
    "        embeddings.append({  \n",
    "            \"embedding\": response.data[0].embedding,  \n",
    "            \"metadata\": doc[\"metadata\"]  \n",
    "        })  \n",
    "        print(f\"Embeddings: {response.data[0].embedding}\")  \n",
    "      \n",
    "    print(\"Embeddings generation complete.\")  \n",
    "      \n",
    "    # Create Search Index  \n",
    "    print(\"Creating search index...\")  \n",
    "    credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\"))  \n",
    "    search_index_client = SearchIndexClient(endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"), credential=credential)  \n",
    "    fields = [  \n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),  \n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),  \n",
    "        SearchableField(name=\"blob_name\", type=SearchFieldDataType.String),  \n",
    "        SearchableField(name=\"document_link\", type=SearchFieldDataType.String),  \n",
    "        SearchField(  \n",
    "            name=\"embedding\",  \n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  \n",
    "            searchable=True,  \n",
    "            vector_search_dimensions=1536,  \n",
    "            vector_search_profile_name=\"myHnswProfile\"  \n",
    "        )  \n",
    "    ]  \n",
    "    vector_search = VectorSearch(  \n",
    "        algorithms=[  \n",
    "            HnswAlgorithmConfiguration(name=\"myHnsw\")  \n",
    "        ],  \n",
    "        profiles=[  \n",
    "            VectorSearchProfile(  \n",
    "                name=\"myHnswProfile\",  \n",
    "                algorithm_configuration_name=\"myHnsw\"  \n",
    "            )  \n",
    "        ]  \n",
    "    )  \n",
    "    index = SearchIndex(name=\"documents-index\", fields=fields, vector_search=vector_search)  \n",
    "    search_index_client.create_index(index)  \n",
    "    print(\"Search index created.\")  \n",
    "      \n",
    "    # Upload chunks and embeddings to Azure AI Search  \n",
    "    print(\"Uploading documents to search index...\")  \n",
    "    search_client = SearchClient(endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"), index_name=\"documents-index\", credential=credential)  \n",
    "    documents_to_upload = []  \n",
    "      \n",
    "    for i, doc in enumerate(embeddings):  \n",
    "        documents_to_upload.append({  \n",
    "            \"id\": str(i),  \n",
    "            \"content\": documents[i][\"text\"],  \n",
    "            \"embedding\": doc[\"embedding\"],  \n",
    "            \"blob_name\": doc[\"metadata\"][\"blob_name\"],  \n",
    "            \"document_link\": doc[\"metadata\"][\"document_link\"]  \n",
    "        })  \n",
    "    search_client.upload_documents(documents=documents_to_upload)  \n",
    "    print(\"Documents uploaded to search index.\")  \n",
    "\n",
    "vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Setup Connections**:\n",
    "    - Connect to Azure OpenAI and Blob Storage.\n",
    "2. **Process Blobs**:\n",
    "    - List blobs in the container.\n",
    "    - For each PDF blob, load its content and split it into chunks with metadata.\n",
    "3. **Customize Metadata**:\n",
    "    - Add custom metadata such as the blob file name and blob URL:\n",
    "        ```python\n",
    "        metadata = {\"blob_name\": blob.name, \"document_link\": document_link}  \n",
    "        ```\n",
    "4. **Generate Embeddings**:\n",
    "    - For each chunk, generate embeddings using Azure OpenAI.\n",
    "5. **Create Search Index**:\n",
    "    - Define and create a search index in Azure AI Search.\n",
    "6. **Upload Documents**:\n",
    "    - Upload the chunks and their embeddings to the search index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve \n",
    "\n",
    "In this section, we will implement a function to perform retrieval queries over documents from the Azure AI Search Index using Azure OpenAI for chat capabilities.\n",
    "\n",
    "Objective: \n",
    "1. Retrieve Function: `chat_on_your_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Retrieve Function: chat_on_your_data  \n",
    "Perform retrieval queries over documents from the Azure AI Search Index using Azure OpenAI. Construct a search query, interacts with the search index, and processes the results to provide relevant information based on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What year was New York State Route 373 built?\n",
      "Processing...\n",
      "GPT Response: New York State Route 373 was designated in 1930.\n",
      "\n",
      "Citation(s):\n",
      "https://syndatastg.blob.core.windows.net/testcloudlab/New_York_State_Route_373.pdf\n"
     ]
    }
   ],
   "source": [
    "def chat_on_your_data():  \n",
    "    \"\"\"  \n",
    "    Perform retrieval queries over documents from the Azure AI Search Index.  \n",
    "    \"\"\"  \n",
    "    # Define the query and other parameters  \n",
    "    query = \"What year was New York State Route 373 built?\" # Example query\n",
    "    search_index = \"documents-index\"  \n",
    "    messages = []  \n",
    "  \n",
    "    # Configure Azure OpenAI parameters  \n",
    "    azure_endpoint = os.getenv('AZURE_OPENAI_BASE')  \n",
    "    azure_openai_api_key = os.getenv('AZURE_OPENAI_KEY')  \n",
    "    azure_openai_api_version = os.getenv('AZURE_OPENAI_VERSION')  \n",
    "    azure_ada_deployment = os.getenv('AZURE_EMBEDDINGS_DEPLOYMENT')  \n",
    "    azure_gpt_deployment = os.getenv('AZURE_GPT_DEPLOYMENT')  \n",
    "  \n",
    "    # Append user query to chat messages  \n",
    "    messages.append({\"role\": \"user\", \"content\": query})  \n",
    "  \n",
    "    print(f\"User: {query}\")  \n",
    "  \n",
    "    print('Processing...')  \n",
    "    # Initialize the AzureOpenAI client  \n",
    "    client = AzureOpenAI(  \n",
    "        azure_endpoint=azure_endpoint,  \n",
    "        api_key=azure_openai_api_key,  \n",
    "        api_version=azure_openai_api_version,  \n",
    "    )  \n",
    "    # Create a chat completion with Azure OpenAI  \n",
    "    completion = client.chat.completions.create(  \n",
    "        model=azure_gpt_deployment,  \n",
    "        messages=[  \n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information. Ensure the Markdown responses are correctly formatted before responding.\"},  \n",
    "            {\"role\": \"user\", \"content\": query}  \n",
    "        ],  \n",
    "        max_tokens=800,  \n",
    "        temperature=0.7,  \n",
    "        top_p=0.95,  \n",
    "        frequency_penalty=0,  \n",
    "        presence_penalty=0,  \n",
    "        stop=None,  \n",
    "        stream=False,  \n",
    "        extra_body={  \n",
    "            \"data_sources\": [{  \n",
    "                \"type\": \"azure_search\",  \n",
    "                \"parameters\": {  \n",
    "                    \"endpoint\": search_endpoint,  \n",
    "                    \"index_name\": search_index,  \n",
    "                    \"semantic_configuration\": \"default\",  \n",
    "                    \"query_type\": \"vector_simple_hybrid\",  \n",
    "                    \"fields_mapping\": {},  \n",
    "                    \"in_scope\": True,  \n",
    "                    \"role_information\": \"You are an AI assistant that helps people find information.\",  \n",
    "                    \"filter\": None,  \n",
    "                    \"strictness\": 3,  \n",
    "                    \"top_n_documents\": 5,  \n",
    "                    \"authentication\": {  \n",
    "                        \"type\": \"api_key\",  \n",
    "                        \"key\": search_key  \n",
    "                    },  \n",
    "                    \"embedding_dependency\": {  \n",
    "                        \"type\": \"deployment_name\",  \n",
    "                        \"deployment_name\": azure_ada_deployment  \n",
    "                    }  \n",
    "                }  \n",
    "            }]  \n",
    "        }  \n",
    "    )  \n",
    "  \n",
    "    # Extract the response data  \n",
    "    response_data = completion.to_dict()  \n",
    "    ai_response = response_data['choices'][0]['message']['content']  \n",
    "    # Clean up the AI response  \n",
    "    ai_response_cleaned = re.sub(r'\\s+\\.$', '.', re.sub(r'\\[doc\\d+\\]', '', ai_response)) \n",
    "    citation = response_data[\"choices\"][0][\"message\"][\"context\"][\"citations\"][0][\"url\"]  \n",
    "    ai_response_final = f\"{ai_response_cleaned}\\n\\nCitation(s):\\n{citation}\"  \n",
    "  \n",
    "    # Append AI response to chat messages  \n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response_final})  \n",
    "  \n",
    "    print(f\"GPT Response: {ai_response_final}\")  \n",
    "  \n",
    "# Call the function to test it  \n",
    "chat_on_your_data()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Define Query and Search Index**:\n",
    "    - Set up the user query and the name of the search index to be created.\n",
    "    - Default values are provided:\n",
    "        - Example query: `\"What year did the hurricane Irene occur?\"`\n",
    "        - Search index: `\"documents-index\"`\n",
    "2. **Configure Azure OpenAI Parameters**:\n",
    "    - Retrieve necessary configurations and API keys from environment variables.\n",
    "3. **Append User Query**:\n",
    "    - Append the user's query to the chat messages list.\n",
    "4. **Initialize AzureOpenAI Client**:\n",
    "    - Initialize the Azure OpenAI client using the provided endpoint, API key, and API version.\n",
    "5. **Create Chat Completion**:\n",
    "    - Create a chat completion request using Azure OpenAI.\n",
    "    - Specify the model deployment, chat messages, and additional parameters like `max_tokens`, `temperature`, etc.\n",
    "    - Provide extra body parameters to include Azure Search as a data source.\n",
    "        - Extra Body Parameters:\n",
    "            - `endpoint`: The Azure Search endpoint.\n",
    "            - `index_name`: The name of the search index.\n",
    "            - `semantic_configuration`: The semantic search configuration.\n",
    "            - `query_type`: Type of query (e.g., `vector_simple_hybrid`).\n",
    "            - `fields_mapping`: Mapping of fields (if any).\n",
    "            - `role_information`: Information about the role of the assistant.\n",
    "            - `filter`: Any filters to apply to the search (if any).\n",
    "            - `strictness`: Level of strictness for the search.\n",
    "            - `top_n_documents`: Number of top documents to retrieve.\n",
    "            - `authentication`: Authentication details (API key).\n",
    "            - `embedding_dependency`: Embedding deployment details.\n",
    "6. **Extract and Clean Response**:\n",
    "    - Extract the response data from the completion result.\n",
    "    - Clean up the AI response by removing unnecessary characters and formatting it properly.\n",
    "    - Extract the citation URL from the response context.\n",
    "7. **Append AI Response**:\n",
    "    - Append the cleaned AI response to the chat messages list.\n",
    "    - Print the final response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully created a Retrieval-Augmented Generation (RAG) application for documents stored in your Azure Blob Storage Account, using Azure OpenAI and Azure AI Search. At this point, you should have a solid understanding of how to build the logic for vectorizing documents from an Azure Blob Storage container and retrieving those documents in your Azure OpenAI application.\n",
    "\n",
    "### Key Accomplishments:\n",
    "1. **Environment Setup**:\n",
    "    - Initialized Azure OpenAI with the necessary API credentials and configurations.\n",
    "    - Established a connection to Azure Blob Storage to access PDF documents.\n",
    "2. **Vectorize**:\n",
    "    - Implemented a function to split PDF text into manageable chunks with associated metadata.\n",
    "    - Orchestrated the entire vectorization process:\n",
    "        - Setup Azure OpenAI and connected to Azure Blob Storage.\n",
    "        - Retrieved and chunked documents.\n",
    "        - Generated embeddings for each chunk using Azure OpenAI.\n",
    "        - Created a search index in Azure AI Search.\n",
    "        - Uploaded the chunks and their embeddings to Azure AI Search.\n",
    "3. **Retrieve**:\n",
    "    - Implemented a function to perform retrieval queries over the documents indexed in Azure AI Search using Azure OpenAI.\n",
    "    - Executed a user query and performed a search using Azure AI Search.\n",
    "    - Generated a chat completion based on the search results and formatted it for display."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
